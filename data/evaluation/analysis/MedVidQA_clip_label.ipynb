{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_number(filename):\n",
    "    # This extracts numbers from a filename like 'frame_00001.jpg'\n",
    "    return int(filename.replace('frame_', '').replace('.jpg', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(content, save_path):\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(json.dumps(content))\n",
    "def load_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return [json.loads(l.strip(\"\\n\")) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_from_full = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SztsZNp-jDM 24 97 322325\n",
      "OKXoHwkx55c 78 98 90\n",
      "V9j5JkWGwI8 66 306 303\n",
      "V9j5JkWGwI8 205 306 303\n",
      "ehl2MPczYoQ 58 92 285458\n",
      "ehl2MPczYoQ 76 92 285458\n",
      "mMNloo140pU 267 477 474\n",
      "QwhD5UTUW60 125 364 430963\n",
      "QwhD5UTUW60 193 364 430963\n",
      "QwhD5UTUW60 298 364 430963\n",
      "Ehan_VI7p4c 258 723 696\n",
      "Ehan_VI7p4c 346 723 696\n",
      "Ehan_VI7p4c 354 723 696\n",
      "Ehan_VI7p4c 411 723 696\n",
      "Ehan_VI7p4c 465 723 696\n",
      "Ehan_VI7p4c 511 723 696\n",
      "Ehan_VI7p4c 587 723 696\n",
      "Ehan_VI7p4c 595 723 696\n",
      "Ehan_VI7p4c 658 723 696\n",
      "19 66 2710 155 145 2625\n",
      "['SztsZNp-jDM', 'OKXoHwkx55c', 'V9j5JkWGwI8', 'ehl2MPczYoQ', 'mMNloo140pU', 'QwhD5UTUW60', 'Ehan_VI7p4c']\n",
      "total number of videos used : 762\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "if extract_from_full:\n",
    "    file_dict = {'image_path':[], 'video':[] }\n",
    "    \n",
    "    for root, dirs, files in os.walk(\"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/medvidqa/raw_frames\"):\n",
    "        for dir in dirs:\n",
    "            for r, d, file in os.walk(os.path.join(root, dir)):\n",
    "                file_dict['image_path'].append(os.path.join(r, file[random.randrange(len(file))])) \n",
    "                file_dict['video'].append(dir)\n",
    "else:\n",
    "    unmatching_frames_cnt, unavailable_cnt = 0, 0\n",
    "    list_of_bad_videos = []\n",
    "    vid_list = []\n",
    "    file_dict = {'image_path':[], 'video':[], 'query':[] }\n",
    "    train_path = '/home/hlpark/shared/MedVidQA/train.json'\n",
    "    val_path = '/home/hlpark/shared/MedVidQA/val.json'\n",
    "    test_path = '/home/hlpark/shared/MedVidQA/test.json'\n",
    "    val = json.load(open(val_path))\n",
    "    test = json.load(open(test_path))\n",
    "    train = json.load(open(train_path))\n",
    "    root = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/medvidqa/raw_frames\"\n",
    "    for idx, v in enumerate(train):\n",
    "        #some videos were deleted from youtube, missing features\n",
    "        if not os.path.exists(os.path.join(root, v['video_id'] +  \".mp4\")):\n",
    "            unavailable_cnt += 1\n",
    "            continue\n",
    "        filenames = os.listdir(os.path.join(root, v['video_id'] +  \".mp4\"))\n",
    "        sorted_filenames = sorted(filenames, key=get_frame_number)\n",
    "        #print(sorted_filenames)\n",
    "        if math.isnan(v['answer_start_second']) or math.isnan(v['answer_end_second']):\n",
    "            print(\"nan detected\")\n",
    "            k = random.randint(0, len(filenames) - 1)\n",
    "        elif int(v['answer_end_second']) > v['video_length'] or int(v['answer_start_second']) > v['video_length']:\n",
    "            print(f\"TIMESTAMP ERROR in original json {v['video_id']} with video length {v['video_length']}: {v['answer_start_second'], v['answer_end_second']}\")\n",
    "            k = random.randint(0, len(filenames) - 1)\n",
    "        else:\n",
    "            k = random.randint(int(v['answer_start_second']), int(v['answer_end_second']))\n",
    "        if abs(v['video_length'] - len(filenames)) > 2:\n",
    "            unmatching_frames_cnt += 1\n",
    "            if v['video_id'] not in list_of_bad_videos:\n",
    "                list_of_bad_videos.append(v['video_id'])\n",
    "            print(v['video_id'], k, v['video_length'], len(filenames))\n",
    "        else:\n",
    "            if v['video_id'] not in vid_list:\n",
    "                vid_list.append(v['video_id'])\n",
    "            file_dict['image_path'].append(os.path.join(root, v['video_id'] +  '.mp4', sorted_filenames[k]))\n",
    "            file_dict['video'].append(v['video_id'])\n",
    "            file_dict['query'].append(v['question'])\n",
    "    print(unmatching_frames_cnt, unavailable_cnt, len(train), len(test), len(val), len(file_dict['image_path']))\n",
    "    print(list_of_bad_videos)\n",
    "    print(f\"total number of videos used : {len(vid_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/evaluation/analysis/MedVidQA\"):\n",
    "    os.makedirs(\"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/evaluation/analysis/MedVidQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a hospital', 'This is a studio', 'This is a home', 'This is a school', 'This is a outside']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2625/2625 [00:34<00:00, 75.29it/s] \n"
     ]
    }
   ],
   "source": [
    "## Caption embeddings\n",
    "# texts = ['hospital', 'office', 'home', 'school', 'outside', 'not hospital']        # FIXME fill in classes here (e.g., hospital, not hospital)\n",
    "# name = \"six_labels\"\n",
    "texts = ['hospital', 'studio', 'home', 'school', 'outside']        # FIXME fill in classes here (e.g., hospital, not hospital)\n",
    "name = \"five_labels_train\"\n",
    "# texts = ['hospital', 'not hospital']        # FIXME fill in classes here (e.g., hospital, not hospital)\n",
    "# name = \"two_labels\"\n",
    "\n",
    "if not os.path.exists(f\"./MedVidQA/txt_probs_{name}.pt\"):\n",
    "\n",
    "    model, preprocess = clip.load(\"ViT-B/32\")\n",
    "    model.cuda().eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "    prob_dict = {}\n",
    "\n",
    "    captions = [\"This is a \" + desc for desc in texts]\n",
    "    print(captions)\n",
    "    text_tokens = clip.tokenize(captions).cuda()\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    batch_size = 500\n",
    "    images, img_feats, txt_probs = [], [], []\n",
    "    for i, x in tqdm(data.iterrows(), total=data.shape[0]):     # FIXME make this iterate over your data\n",
    "\n",
    "        path = x['image_path']        # FIXME replace with the frame from the appropriate video\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        images.append(preprocess(image))\n",
    "        \n",
    "        if ((i + 1) % batch_size == 0) or (i + 1 == data.shape[0]):\n",
    "            ## Image embeddings\n",
    "            image_input = torch.tensor(np.stack(images)).cuda()\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(image_input).float()\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                img_feats.append(image_features.cpu())\n",
    "\n",
    "            ## Caption probabilities\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "            txt_probs.append(text_probs.cpu())\n",
    "            images = []\n",
    "\n",
    "    img_feats = torch.cat(img_feats, dim=0)\n",
    "    txt_probs = torch.cat(txt_probs, dim=0)\n",
    "    torch.save(img_feats, f'MedVidQA/img_feats_{name}.pt')\n",
    "    torch.save(txt_probs, f'MedVidQA/txt_probs_{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8294, 0.0583, 0.0635, 0.0153, 0.0336],\n",
      "        [0.2339, 0.4441, 0.0530, 0.0247, 0.2442],\n",
      "        [0.2683, 0.5058, 0.0616, 0.0579, 0.1064],\n",
      "        ...,\n",
      "        [0.3533, 0.3429, 0.1499, 0.0216, 0.1322],\n",
      "        [0.7830, 0.1075, 0.0420, 0.0262, 0.0413],\n",
      "        [0.6810, 0.1176, 0.0598, 0.0695, 0.0721]])\n",
      "1\n",
      "2625\n",
      "1198\n",
      "0 0 1198 1427\n",
      "medical video numbers:  526 non-medical video numbers:  449\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import Image, display\n",
    "from PIL import Image\n",
    "label  = \"five_labels_train\"\n",
    "probs = torch.load(f\"./MedVidQA/txt_probs_{label}.pt\")\n",
    "cnt = 0\n",
    "med_vid_list, non_med_vid_list = [], []\n",
    "label_dict = {}\n",
    "print(probs)\n",
    "for p in probs[0]:\n",
    "    if np.argmax(p) == 0 and p > 0.8:\n",
    "        cnt += 1\n",
    "\n",
    "print(cnt)\n",
    "\n",
    "root = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/evaluation/analysis/MedVidQA\"\n",
    "if not os.path.exists(os.path.join(root, f\"{label}_classification_images/\")):\n",
    "    os.makedirs(os.path.join(root, f\"{label}_classification_images\"))\n",
    "    os.makedirs(os.path.join(root, f\"{label}_classification_images/{label}_mm\"))\n",
    "    os.makedirs(os.path.join(root, f\"{label}_classification_images/{label}_nmm\"))\n",
    "    os.makedirs(os.path.join(root, f\"{label}_classification_images/{label}_mnm\"))\n",
    "    os.makedirs(os.path.join(root, f\"{label}_classification_images/{label}_nmnm\"))\n",
    "max_idx = np.argmax(probs, 1)\n",
    "max_idx = max_idx.tolist()\n",
    "print(len(max_idx))\n",
    "print(max_idx.count(0))\n",
    "medical_tv_class_med, medical_tv_class_nonmed = 0, 0\n",
    "nonmedical_tv_class_med, nonmedical_tv_class_nonmed = 0, 0\n",
    "for i,x in enumerate(max_idx):\n",
    "    img = Image.open(data.iloc[i]['image_path'])\n",
    "    if data.iloc[i]['video'] not in label_dict:\n",
    "        label_dict[data.iloc[i]['video']] = []\n",
    "    if x == 0:\n",
    "        label_dict[data.iloc[i]['video']].append({data.iloc[i]['query'] : \"med\"})\n",
    "        if data.iloc[i]['video'] not in med_vid_list:\n",
    "            med_vid_list.append(data.iloc[i]['video'])\n",
    "        if \"house\" in data.iloc[i]['video'] or \"grey\" in data.iloc[i]['video']:\n",
    "            medical_tv_class_med += 1\n",
    "            img.save(f\"{root}/{label}_classification_images/{label}_mm/\" + data.iloc[i]['video'] + \".jpg\", 'JPEG')\n",
    "        else:\n",
    "            nonmedical_tv_class_med += 1\n",
    "            img.save(f\"{root}/{label}_classification_images/{label}_nmm/\" + data.iloc[i]['video'] + \".jpg\", 'JPEG')\n",
    "\n",
    "    else:\n",
    "        label_dict[data.iloc[i]['video']].append({data.iloc[i]['query'] : \"nonmed\"})\n",
    "        if data.iloc[i]['video'] not in non_med_vid_list:\n",
    "            non_med_vid_list.append(data.iloc[i]['video'])\n",
    "        if \"house\" in data.iloc[i]['video'] or \"grey\" in data.iloc[i]['video']:\n",
    "            medical_tv_class_nonmed += 1\n",
    "            img.save(f\"{root}/{label}_classification_images/{label}_mnm/\" + data.iloc[i]['video'] + \".jpg\", 'JPEG')\n",
    "\n",
    "        else:\n",
    "            nonmedical_tv_class_nonmed += 1\n",
    "            img.save(f\"{root}/{label}_classification_images/{label}_nmnm/\" + data.iloc[i]['video'] + \".jpg\", 'JPEG')\n",
    "\n",
    "\n",
    "print(medical_tv_class_med, medical_tv_class_nonmed, nonmedical_tv_class_med, nonmedical_tv_class_nonmed)\n",
    "print(\"medical video numbers: \", len(med_vid_list), \"non-medical video numbers: \", len(non_med_vid_list))\n",
    "save_json(label_dict, \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/splits/medvidqa/five_labeled_pred_med_from_gt_vid_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hirest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
