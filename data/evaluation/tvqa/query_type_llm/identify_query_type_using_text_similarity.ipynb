{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pysrt\n",
    "import re\n",
    "import shutil\n",
    "from openai import OpenAI, RateLimitError\n",
    "import yaml\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"tvqa_gpt_query_type_textsim_experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select queries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(phrase, paragraph):\n",
    "    # Search for exact match\n",
    "    exact_match = re.search(phrase, paragraph)\n",
    "    if exact_match:\n",
    "        print(\"Exact match found:\", exact_match.group())\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "def semantic_similarity(sentences, query):\n",
    "    \n",
    "    embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "    #embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    sentences_embeddings = embedder.encode(sentences, convert_to_tensor=True)\n",
    "    top_k = min(5, len(sentences))\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, sentences_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "    max_score = 0\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(sentences[idx], \"(Score: {:.4f})\".format(score))\n",
    "        if max_score < score:\n",
    "            max_score = score\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: He suicide\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "The person electrocute himself (Score: 0.4032)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: he was sick\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Nope. Not only that, my sight has shifted from clear to milky green.' (Score: 0.0320)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: sick\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "and I am producing sputum at an alarming rate. (Score: 0.1575)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: upset\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "You don't believe in me (Score: 0.2089)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2089, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_similarity([\"The person electrocute himself\"], \"He suicide\")\n",
    "semantic_similarity([\"Nope. Not only that, my sight has shifted from clear to milky green.'\"], \"he was sick\")\n",
    "semantic_similarity([\"and I am producing sputum at an alarming rate.\"], \"sick\")\n",
    "semantic_similarity([\"You don't believe in me\"], \"upset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(content, save_path):\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(json.dumps(content))\n",
    "def load_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return [json.loads(l.strip(\"\\n\")) for l in f.readlines()]\n",
    "def load_result_json(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_json_folder = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/splits/tvqa\"\n",
    "val = load_jsonl(f'{vid_json_folder}/all_data_val.json')\n",
    "test = load_jsonl(f'{vid_json_folder}/all_data_test.json')\n",
    "train = load_jsonl(f'{vid_json_folder}/all_data_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_duration_json = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/splits/tvqa/video_duration.json\"\n",
    "video_duration_dict = load_jsonl(vid_duration_json)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/tvqa/ASR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"paragraph_semantic_search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load srt files and extract ground truth scripts\n",
    "import pysrt\n",
    "import numpy as np\n",
    "json_save_folder = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/splits\"\n",
    "json_folder = os.path.join(json_save_folder, experiment_name, 'baseline')\n",
    "if os.path.exists(f'{json_folder}/val_audio_relevance_score_{option}.json'):\n",
    "    val_json = load_jsonl(f'{json_folder}/val_audio_relevance_score_{option}.json')\n",
    "    val_srt = val_json[0]\n",
    "else:\n",
    "    val_srt = []\n",
    "    for key, value in val[0].items():\n",
    "        qa_dict = {}\n",
    "        gt_timestamp = value[list(value.keys())[0]]['bounds'] \n",
    "        gt_timestamp_start, gt_timestamp_end = float(gt_timestamp[0]), float(gt_timestamp[1])\n",
    "        if math.isnan(gt_timestamp_start) or math.isnan(gt_timestamp_end):\n",
    "            gt_timestamp_start = 0\n",
    "            gt_timestamp_end = value[list(value.keys())[0]]['v_duration']\n",
    "        \n",
    "        sub_list = []\n",
    "        subs =  pysrt.open(os.path.join(root, list(value.keys())[0].replace(\".mp4\", \"\") + \".srt\"))\n",
    "        for sub in subs:\n",
    "            if (sub.start.seconds + sub.start.minutes * 60  >= int(gt_timestamp_start)) and (sub.start.seconds + sub.start.minutes * 60  <= int(np.ceil(gt_timestamp_end))):\n",
    "                #print(sub.start, sub.text)\n",
    "                sub_list.append({str(sub.start) : sub.text})\n",
    "        qa_dict['question'] = key\n",
    "        #print(value)\n",
    "        qa_dict['answer'] = value['answer']\n",
    "        qa_dict['video'] = list(value.keys())[0]\n",
    "        qa_dict['sub'] = sub_list\n",
    "        val_srt.append(qa_dict)\n",
    "\n",
    "\n",
    "idx = 0\n",
    "for value in val_srt:\n",
    "    sentences = []\n",
    "    prompt = \"\"\n",
    "    if len(value['sub']) == 0:\n",
    "        continue    \n",
    "    if 'score' in value:\n",
    "        continue\n",
    "    for idx, sentence in enumerate(value['sub']):\n",
    "        if option == \"paragraph_semantic_search\" or option == \"exact_match\":\n",
    "            prompt += f\"\\n{list(sentence.values())[0]}\"\n",
    "        elif option == \"sentence_semantic_search_max\":\n",
    "            sentences.append(list(sentence.values())[0])\n",
    "        \n",
    "    if option == \"exact_match\":\n",
    "        if find_match(value['answer'], prompt):\n",
    "            print(\"Q: \", value['question'], '\\n', value['answer'], '\\n', prompt)\n",
    "            value['score'] = 3\n",
    "        else:\n",
    "            value['score'] = 1\n",
    "    else:\n",
    "        ret = semantic_similarity([prompt], value['question'])\n",
    "        ret = ret.item()\n",
    "        value['score'] = ret * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load srt files and extract ground truth scripts\n",
    "import pysrt\n",
    "import numpy as np\n",
    "json_save_folder = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/splits\"\n",
    "json_folder = os.path.join(json_save_folder, experiment_name, 'baseline')\n",
    "if os.path.exists(f'{json_folder}/test_audio_relevance_score_{option}.json'):\n",
    "    test_json = load_jsonl(f'{json_folder}/test_audio_relevance_score_{option}.json')\n",
    "    test_srt = test_json[0]\n",
    "else:\n",
    "    test_srt = []\n",
    "    for key, value in test[0].items():\n",
    "        qa_dict = {}\n",
    "        gt_timestamp = value[list(value.keys())[0]]['bounds'] \n",
    "        gt_timestamp_start, gt_timestamp_end = float(gt_timestamp[0]), float(gt_timestamp[1])\n",
    "        if math.isnan(gt_timestamp_start) or math.isnan(gt_timestamp_end):\n",
    "            gt_timestamp_start = 0\n",
    "            gt_timestamp_end = value[list(value.keys())[0]]['v_duration']\n",
    "        \n",
    "        sub_list = []\n",
    "        subs =  pysrt.open(os.path.join(root, list(value.keys())[0].replace(\".mp4\", \"\") + \".srt\"))\n",
    "        for sub in subs:\n",
    "            if (sub.start.seconds + sub.start.minutes * 60  >= int(gt_timestamp_start)) and (sub.start.seconds + sub.start.minutes * 60  <= int(np.ceil(gt_timestamp_end))):\n",
    "                #print(sub.start, sub.text)\n",
    "                sub_list.append({str(sub.start) : sub.text})\n",
    "        qa_dict['question'] = key\n",
    "        #print(value)\n",
    "        qa_dict['answer'] = value['answer']\n",
    "        qa_dict['video'] = list(value.keys())[0]\n",
    "        qa_dict['sub'] = sub_list\n",
    "        test_srt.append(qa_dict)\n",
    "\n",
    "\n",
    "idx = 0\n",
    "for value in test_srt:\n",
    "    sentences = []\n",
    "    prompt = \"\"\n",
    "    if len(value['sub']) == 0:\n",
    "        continue    \n",
    "    if 'score' in value:\n",
    "        continue\n",
    "    for idx, sentence in enumerate(value['sub']):\n",
    "        if option == \"paragraph_semantic_search\" or option == \"exact_match\":\n",
    "            prompt += f\"\\n{list(sentence.values())[0]}\"\n",
    "        elif option == \"sentence_semantic_search_max\":\n",
    "            sentences.append(list(sentence.values())[0])\n",
    "        \n",
    "    if option == \"exact_match\":\n",
    "        if find_match(value['answer'], prompt):\n",
    "            print(\"Q: \", value['question'], '\\n', value['answer'], '\\n', prompt)\n",
    "            value['score'] = 3\n",
    "        else:\n",
    "            value['score'] = 1\n",
    "    else:\n",
    "        ret = semantic_similarity([prompt], value['question'])\n",
    "        ret = ret.item()\n",
    "        value['score'] = ret * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load srt files and extract ground truth scripts\n",
    "import pysrt\n",
    "import numpy as np\n",
    "\n",
    "json_save_folder = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/splits\"\n",
    "json_folder = os.path.join(json_save_folder, experiment_name, 'baseline')\n",
    "if os.path.exists(f'{json_folder}/train_audio_relevance_score_{option}.json'):\n",
    "    train_json = load_jsonl(f'{json_folder}/train_audio_relevance_score_{option}.json')\n",
    "    train_srt = train_json[0]\n",
    "else:\n",
    "    train_srt = []\n",
    "    for key, value in train[0].items():\n",
    "        qa_dict = {}\n",
    "        gt_timestamp = value[list(value.keys())[0]]['bounds'] \n",
    "        gt_timestamp_start, gt_timestamp_end = float(gt_timestamp[0]), float(gt_timestamp[1])\n",
    "        if math.isnan(gt_timestamp_start) or math.isnan(gt_timestamp_end):\n",
    "            gt_timestamp_start = 0\n",
    "            gt_timestamp_end = value[list(value.keys())[0]]['v_duration']\n",
    "        \n",
    "        sub_list = []\n",
    "        subs =  pysrt.open(os.path.join(root, list(value.keys())[0].replace(\".mp4\", \"\") + \".srt\"))\n",
    "        for sub in subs:\n",
    "            if (sub.start.seconds + sub.start.minutes * 60  >= int(gt_timestamp_start)) and (sub.start.seconds + sub.start.minutes * 60  <= int(np.ceil(gt_timestamp_end))):\n",
    "                #print(sub.start, sub.text)\n",
    "                sub_list.append({str(sub.start) : sub.text})\n",
    "        qa_dict['question'] = key\n",
    "        #print(value)\n",
    "        qa_dict['answer'] = value['answer']\n",
    "        qa_dict['video'] = list(value.keys())[0]\n",
    "        qa_dict['sub'] = sub_list\n",
    "        train_srt.append(qa_dict)\n",
    "\n",
    "idx = 0\n",
    "for value in train_srt:\n",
    "    sentences = []\n",
    "    prompt = \"\"\n",
    "    if len(value['sub']) == 0:\n",
    "        continue    \n",
    "    \n",
    "    if 'score' in value:\n",
    "        continue\n",
    "\n",
    "    for idx, sentence in enumerate(value['sub']):\n",
    "        if option == \"paragraph_semantic_search\" or option == \"exact_match\":\n",
    "            prompt += f\"\\n{list(sentence.values())[0]}\"\n",
    "        elif option == \"sentence_semantic_search_max\":\n",
    "            sentences.append(list(sentence.values())[0])\n",
    "        \n",
    "    if option == \"exact_match\":\n",
    "        if find_match(value['answer'], prompt):\n",
    "            print(\"Q: \", value['question'], '\\n', value['answer'], '\\n', prompt)\n",
    "            value['score'] = 3\n",
    "        else:\n",
    "            value['score'] = 1\n",
    "    else:\n",
    "        ret = semantic_similarity([prompt], value['question'])\n",
    "        ret = ret.item()\n",
    "        value['score'] = ret * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder for each testing set (asr features have to be seprate folder as well)\n",
    "json_save_folder = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/splits\"\n",
    "with open(f'{vid_json_folder}/all_data_val.json') as f:\n",
    "    val_json = json.load(f)\n",
    "with open(f'{vid_json_folder}/all_data_test.json') as f:\n",
    "    test_json = json.load(f)\n",
    "with open(f'{vid_json_folder}/all_data_train.json') as f:\n",
    "    train_json = json.load(f)\n",
    "\n",
    "exp_list = ['baseline', 'visual_med', 'visual_med_with_audio', 'visual_nonmed']\n",
    "\n",
    "for exp in exp_list:\n",
    "    json_folder = os.path.join(json_save_folder, experiment_name, exp)\n",
    "\n",
    "    if not os.path.exists(json_folder):\n",
    "        os.makedirs(json_folder)\n",
    "\n",
    "    save_json(test_json, f'{json_folder}/all_data_test.json')\n",
    "    save_json(val_json, f'{json_folder}/all_data_val.json')\n",
    "    save_json(train_json, f'{json_folder}/all_data_train.json')\n",
    "    save_json(val_srt, f'{json_folder}/val_audio_relevance_score_{option}.json')\n",
    "    save_json(train_srt, f'{json_folder}/train_audio_relevance_score_{option}.json')\n",
    "    save_json(test_srt, f'{json_folder}/test_audio_relevance_score_{option}.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/home/hlpark/REDUCE/REDUCE_benchmarks/HiREST/data/splits/tvqa\"\n",
    "save_json(train_srt, f'{json_folder}/train_audio_relevance_score_{option}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hirest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
